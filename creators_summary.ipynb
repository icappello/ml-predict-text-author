{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v24tePJBsFbe"
   },
   "source": [
    "# Disclaimer\n",
    "\n",
    "Released under the CC BY 4.0 License (https://creativecommons.org/licenses/by/4.0/)\n",
    "\n",
    "# Purpose of this notebook\n",
    "\n",
    "The purpose of this document is to show how I approached the presented problem and to record my learning experience in how to use Tensorflow 2 and CatBoost to perform a classification task on text data.\n",
    "\n",
    "If, while reading this document, you think _\"Why didn't you do `<this>` instead of `<that>`?\"_, the answer could be simply because I don't know about `<this>`. Comments, questions and constructive criticism are of course welcome.\n",
    "\n",
    "# Intro\n",
    "\n",
    "This simple classification task has been developed to get familiarized with Tensorflow 2 and CatBoost handling of text data. In summary, the task is to predict the author of a short text.\n",
    "\n",
    "To get a number of train/test examples, it is enough to create a twitter app and, using the python client library for twitter, read the user timeline of multiple accounts. This process is not covered here. If you are interested in this topic, feel free to contact me.\n",
    "\n",
    "\n",
    "## Features\n",
    "\n",
    "It is assumed the collected raw data consists of:\n",
    "\n",
    "1.   The author handle (the label that will be predicted)\n",
    "2.   The timestamp of the post\n",
    "3.   The raw text of the post\n",
    "\n",
    "### Preparing the dataset\n",
    "\n",
    "When preparing the dataset, the content of the post is preprocessed using these rules:\n",
    "\n",
    "1.   Newlines are replaced with a space\n",
    "2.   Links are replaced with a placeholder (e.g. `<link>`)\n",
    "3.   For each possible unicode char category, the number of chars in that category is added as a feature\n",
    "4.   The number of words for each tweet is added as a feature\n",
    "5.   Retweets (even retweets with comment) are discarded. Only responses and original tweets are taken into account\n",
    "\n",
    "The dataset has been randomly split into three different files for train (70%), validation (10%) and test (20%). For each label, it has been verified that the same percentages hold in all three files.\n",
    "\n",
    "Before fitting the data and before evaluation on the test dataset, the timestamp values are normalized, using the mean and standard deviation computed on the train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cixKTRjUQfJ0"
   },
   "source": [
    "# TensorFlow 2 model\n",
    "\n",
    "The model has four different input features:\n",
    "\n",
    "1.    The normalized timestamp.\n",
    "2.    The input text, represented as the whole sentence. This will be transformed in a 128-dimensional vector by an embedding layer.\n",
    "3.    The input text, this time represented as a sequence of words, expressed as indexes of tokens. This representation will be used by a LSTM layer to try to extract some meaning from the actual sequence of the used words.\n",
    "4.    The unicode character category usage. This should help in identify handles that use emojis, a lot of punctuation or unusual chars.\n",
    "\n",
    "The resulting layers are concatenated, then after a sequence of two dense layers (with an applied dropout) the final layer computes the logits for the different classes. The used loss function is *sparse categorical crossentropy*, since the labels are represented as indexes of a list of twitter handles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AJdXtG7eQ5tK"
   },
   "source": [
    "## Imports for the TensorFlow 2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Dl5RBSRQ31X"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import Input, layers\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "import calendar\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import unicodedata\n",
    "#masking layers and GPU don't mix\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bzzxApOjQ_bJ"
   },
   "source": [
    "## Definitions for the TensorFlow 2 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABqLUusIRMAs"
   },
   "outputs": [],
   "source": [
    "#Download size: ~446MB\n",
    "hub_layer = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\",\n",
    "    output_shape=[512],\n",
    "    input_shape=[],\n",
    "    dtype=tf.string,\n",
    "    trainable=False\n",
    ")\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\")\n",
    "\n",
    "unicode_data_categories = [\n",
    "    \"Cc\",\n",
    "    \"Cf\",\n",
    "    \"Cn\",\n",
    "    \"Co\",\n",
    "    \"Cs\",\n",
    "    \"LC\",\n",
    "    \"Ll\",\n",
    "    \"Lm\",\n",
    "    \"Lo\",\n",
    "    \"Lt\",\n",
    "    \"Lu\",\n",
    "    \"Mc\",\n",
    "    \"Me\",\n",
    "    \"Mn\",\n",
    "    \"Nd\",\n",
    "    \"Nl\",\n",
    "    \"No\",\n",
    "    \"Pc\",\n",
    "    \"Pd\",\n",
    "    \"Pe\",\n",
    "    \"Pf\",\n",
    "    \"Pi\",\n",
    "    \"Po\",\n",
    "    \"Ps\",\n",
    "    \"Sc\",\n",
    "    \"Sk\",\n",
    "    \"Sm\",\n",
    "    \"So\",\n",
    "    \"Zl\",\n",
    "    \"Zp\",\n",
    "    \"Zs\"\n",
    "]\n",
    "\n",
    "column_names = [\n",
    "    \"handle\",\n",
    "    \"timestamp\",\n",
    "    \"text\"\n",
    "]\n",
    "\n",
    "column_names.extend(unicode_data_categories)\n",
    "\n",
    "train_file = os.path.realpath(\"input.csv\")\n",
    "\n",
    "n_tokens = 100000\n",
    "\n",
    "tokenizer = Tokenizer(n_tokens, oov_token='<OOV>')\n",
    "\n",
    "#List of handles (labels)\n",
    "#Fill with the handles you want to consider in your dataset\n",
    "handles = [\n",
    "\n",
    "]\n",
    "\n",
    "end_token = \"XEND\"\n",
    "\n",
    "train_file = os.path.realpath(\"data/train.csv\")\n",
    "val_file = os.path.realpath(\"data/val.csv\")\n",
    "test_file = os.path.realpath(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TIQrVrIyR1BB"
   },
   "source": [
    "## Preprocessing and computing dataset features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X6v13WJXR2qq"
   },
   "outputs": [],
   "source": [
    "def get_pandas_dataset(input_file, fit_tokenizer=False, timestamp_mean=None, timestamp_std=None, pad_sequence=None):\n",
    "\n",
    "    pd_dat = pd.read_csv(input_file, names=column_names)\n",
    "    \n",
    "    pd_dat = pd_dat[pd_dat.handle.isin(handles)]\n",
    "\n",
    "    if(timestamp_mean is None):\n",
    "         timestamp_mean = pd_dat.timestamp.mean()\n",
    "    \n",
    "    if(timestamp_std is None):\n",
    "        timestamp_std = pd_dat.timestamp.std()\n",
    "\n",
    "    pd_dat.timestamp = (pd_dat.timestamp - timestamp_mean) / timestamp_std\n",
    "\n",
    "    pd_dat[\"handle_index\"] = pd_dat['handle'].map(lambda x: handles.index(x))\n",
    "\n",
    "    if(fit_tokenizer):\n",
    "        tokenizer.fit_on_texts(pd_dat[\"text\"])\n",
    "        pad_sequence = tokenizer.texts_to_sequences([[end_token]])[0][0]\n",
    "    \n",
    "    pd_dat[\"sequence\"] = tokenizer.texts_to_sequences(pd_dat[\"text\"])\n",
    "\n",
    "\n",
    "    max_seq_length = 30\n",
    "    pd_dat = pd_dat.reset_index(drop=True)\n",
    "\n",
    "    #max length\n",
    "    pd_dat[\"sequence\"] = pd.Series(el[0:max_seq_length] for el in pd_dat[\"sequence\"])\n",
    "\n",
    "    #padding\n",
    "    pd_dat[\"sequence\"] = pd.Series([el + ([pad_sequence] * (max_seq_length - len(el))) for el in pd_dat[\"sequence\"]])\n",
    "    \n",
    "    pd_dat[\"words_in_tweet\"] = pd_dat[\"text\"].str.strip().str.split(\" \").str.len() + 1\n",
    "    \n",
    "    return pd_dat, timestamp_mean, timestamp_std, pad_sequence\n",
    "\n",
    "train_dataset, timestamp_mean, timestamp_std, pad_sequence = get_pandas_dataset(train_file, fit_tokenizer=True)\n",
    "\n",
    "test_dataset, _, _, _= get_pandas_dataset(test_file, timestamp_mean=timestamp_mean, timestamp_std=timestamp_std, pad_sequence=pad_sequence)\n",
    "\n",
    "val_dataset, _, _, _ = get_pandas_dataset(val_file, timestamp_mean=timestamp_mean, timestamp_std=timestamp_std, pad_sequence=pad_sequence)\n",
    "\n",
    "#selecting as features only the unicode categories that are used in the train dataset\n",
    "non_null_unicode_categories = []\n",
    "for unicode_data_category in unicode_data_categories:\n",
    "    category_name = unicode_data_category\n",
    "    category_sum = train_dataset[category_name].sum()\n",
    "\n",
    "    if(category_sum > 0):\n",
    "        non_null_unicode_categories.append(category_name) \n",
    "\n",
    "print(\"Bucketized unicode categories used as features: \" + repr(non_null_unicode_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dh1eTHn-TY9b"
   },
   "source": [
    "## Defining input/output features from the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ItvQq-OlTitb"
   },
   "outputs": [],
   "source": [
    "def split_inputs_and_outputs(pd_dat):\n",
    "\n",
    "\n",
    "    labels = pd_dat['handle_index'].values\n",
    "\n",
    "    icolumns = pd_dat.columns\n",
    "\n",
    "    timestamps = pd_dat.loc[:, \"timestamp\"].astype(np.float32)\n",
    "    text = pd_dat.loc[:, \"text\"]\n",
    "    sequence = np.asarray([np.array(el) for el in pd_dat.loc[:, \"sequence\"]])\n",
    "    #unicode_char_ratios = pd_dat[unicode_data_categories].astype(np.float32)\n",
    "    unicode_char_categories = {\n",
    "        category_name: pd_dat[category_name] for category_name in non_null_unicode_categories\n",
    "    }\n",
    "    \n",
    "    words_in_tweet = pd_dat['words_in_tweet']\n",
    "\n",
    "    return timestamps, text, sequence, unicode_char_categories, words_in_tweet, labels\n",
    "\n",
    "timestamps_train, text_train, sequence_train, unicode_char_categories_train, words_in_tweet_train, labels_train = split_inputs_and_outputs(train_dataset)\n",
    "timestamps_val, text_val, sequence_val, unicode_char_categories_val, words_in_tweet_val, labels_val = split_inputs_and_outputs(val_dataset)\n",
    "timestamps_test, text_test, sequence_test, unicode_char_categories_test, words_in_tweet_test, labels_test = split_inputs_and_outputs(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cc7CEeVgTsWB"
   },
   "source": [
    "## Input tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sad1qGeyTtvi"
   },
   "outputs": [],
   "source": [
    "input_timestamp = Input(shape=(1, ), name='input_timestamp', dtype=tf.float32)\n",
    "input_text = Input(shape=(1, ), name='input_text', dtype=tf.string)\n",
    "input_sequence = Input(shape=(None, 1 ), name=\"input_sequence\", dtype=tf.float32)\n",
    "input_unicode_char_categories = [\n",
    "    Input(shape=(1, ), name=\"input_\"+category_name, dtype=tf.float32) for category_name in non_null_unicode_categories\n",
    "]\n",
    "input_words_in_tweet = Input(shape=(1, ), name=\"input_words_in_tweet\", dtype=tf.float32)\n",
    "\n",
    "inputs_train = {\n",
    "    'input_timestamp': timestamps_train,\n",
    "    \"input_text\": text_train,\n",
    "    \"input_sequence\": sequence_train,\n",
    "    'input_words_in_tweet': words_in_tweet_train,\n",
    "}\n",
    "\n",
    "inputs_train.update({\n",
    "    'input_' + category_name: unicode_char_categories_train[category_name] for category_name in non_null_unicode_categories\n",
    "})\n",
    "\n",
    "outputs_train = labels_train\n",
    "\n",
    "inputs_val = {\n",
    "    'input_timestamp': timestamps_val,\n",
    "    \"input_text\": text_val,\n",
    "    \"input_sequence\": sequence_val,\n",
    "    'input_words_in_tweet': words_in_tweet_val\n",
    "}\n",
    "\n",
    "inputs_val.update({\n",
    "    'input_' + category_name: unicode_char_categories_val[category_name] for category_name in non_null_unicode_categories\n",
    "})\n",
    "          \n",
    "outputs_val = labels_val\n",
    "\n",
    "inputs_test = {\n",
    "    'input_timestamp': timestamps_test,\n",
    "    \"input_text\": text_test,\n",
    "    \"input_sequence\": sequence_test,\n",
    "    'input_words_in_tweet': words_in_tweet_test\n",
    "}\n",
    "\n",
    "inputs_test.update({\n",
    "    'input_' + category_name: unicode_char_categories_test[category_name] for category_name in non_null_unicode_categories\n",
    "})\n",
    "\n",
    "outputs_test = labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Mo17vnQUBWK"
   },
   "source": [
    "## TensorFlow 2 model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xDlkOVEqUE8j"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "    reg = None\n",
    "    activation = 'relu'\n",
    "\n",
    "    reshaped_text = layers.Reshape(target_shape=())(input_text)\n",
    "    embedded = hub_layer(reshaped_text)\n",
    "    x = layers.Dense(256, activation=activation)(embedded)\n",
    "\n",
    "    masking = layers.Masking(mask_value=pad_sequence)(input_sequence)\n",
    "\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(32))(masking)\n",
    "\n",
    "    flattened_lstm_layer = layers.Flatten()(lstm_layer)\n",
    "\n",
    "    x = layers.concatenate([\n",
    "        input_timestamp, \n",
    "        flattened_lstm_layer,\n",
    "        *input_unicode_char_categories,\n",
    "        input_words_in_tweet,\n",
    "        x\n",
    "    ])\n",
    "\n",
    "    x = layers.Dense(n_tokens // 30, activation=activation, kernel_regularizer=reg)(x)\n",
    "\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    x = layers.Dense(n_tokens // 50, activation=activation, kernel_regularizer=reg)(x)\n",
    "    \n",
    "    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    x = layers.Dense(256, activation=activation, kernel_regularizer=reg)(x)\n",
    "\n",
    "    y = layers.Dense(len(handles), activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[\n",
    "                input_timestamp, \n",
    "                input_text, \n",
    "                input_sequence,\n",
    "                *input_unicode_char_categories,\n",
    "                input_words_in_tweet\n",
    "        ],\n",
    "        outputs=[y]\n",
    "    )\n",
    "\n",
    "    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=cce,\n",
    "        metrics=['sparse_categorical_accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "tf.keras.utils.plot_model(model, to_file='twitstar.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hygiOHkUQBZ"
   },
   "source": [
    "## TensorFlow 2 model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c8TQEVHeUS3k"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    inputs_train,\n",
    "    outputs_train,\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    verbose=True,\n",
    "    validation_data=(inputs_val, outputs_val),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            os.path.realpath(\"weights.h5\"),\n",
    "            monitor=\"val_sparse_categorical_accuracy\", \n",
    "            save_best_only=True,\n",
    "            verbose=2\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            patience=3,\n",
    "            monitor=\"val_sparse_categorical_accuracy\"\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TY8cOfMM03z0"
   },
   "source": [
    "## TensorFlow 2 model plots for train loss and accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ix6BTFsk1MCd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss vs. epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Accuracy vs. epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOvqOuwtUaI8"
   },
   "source": [
    "## TensorFlow 2 model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ujdy_-oyUcbo"
   },
   "outputs": [],
   "source": [
    "#loading the \"best\" weights\n",
    "model.load_weights(os.path.realpath(\"weights.h5\"))\n",
    "\n",
    "model.evaluate(inputs_test, outputs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i5pk9ZfWyDFa"
   },
   "source": [
    "### TensorFlow 2 model confusion matrix\n",
    "\n",
    "Using predictions on the test set, a confusion matrix is produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JjIS-cDPyNQ5"
   },
   "outputs": [],
   "source": [
    "def tf2_confusion_matrix(inputs, outputs):\n",
    "    predictions = model.predict(inputs)\n",
    "    wrong_labelled_counter = np.zeros((len(handles), len(handles)))\n",
    "\n",
    "    wrong_labelled_sequences = np.empty((len(handles), len(handles)), np.object)\n",
    "\n",
    "    for i in range(len(handles)):\n",
    "        for j in range(len(handles)):\n",
    "            wrong_labelled_sequences[i][j] = []\n",
    "\n",
    "    tot_wrong = 0\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        predicted = int(predictions[i].argmax())\n",
    "        \n",
    "        true_value = int(outputs[i])\n",
    "\n",
    "        wrong_labelled_counter[true_value][predicted] += 1\n",
    "        wrong_labelled_sequences[true_value][predicted].append(inputs.get('input_text')[i])\n",
    "        \n",
    "        ok = (int(true_value) == int(predicted))\n",
    "        if(not ok):\n",
    "            tot_wrong += 1\n",
    "\n",
    "    return wrong_labelled_counter, wrong_labelled_sequences, predictions\n",
    "\n",
    "def print_confusion_matrix(wrong_labelled_counter):\n",
    "    the_str = \"\\t\"\n",
    "    for handle in handles:\n",
    "        the_str += handle + \"\\t\"\n",
    "    print(the_str)\n",
    "\n",
    "    ctr = 0\n",
    "    for row in wrong_labelled_counter:\n",
    "        the_str = handles[ctr] + '\\t'\n",
    "        ctr+=1\n",
    "        for i in range(len(row)):\n",
    "            the_str += str(int(row[i]))\n",
    "            if(i != len(row) -1):\n",
    "                the_str += \"\\t\"\n",
    "        print(the_str)\n",
    "\n",
    "wrong_labelled_counter, wrong_labelled_sequences, predictions = tf2_confusion_matrix(inputs_test, outputs_test)\n",
    "\n",
    "\n",
    "print_confusion_matrix(wrong_labelled_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLn-VedFUpwd"
   },
   "source": [
    "# CatBoost model\n",
    "\n",
    "This CatBoost model instance was developed reusing the ideas presented in these tutorials from the official repository: [classification](https://github.com/catboost/tutorials/blob/master/classification/classification_tutorial.ipynb) and [text features](https://github.com/catboost/tutorials/blob/master/text_features/text_features_in_catboost.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEKBxOJ_Wdgy"
   },
   "source": [
    "## Imports for the CatBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3pNFdq1rWe9J"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "import calendar\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "import unicodedata\n",
    "from catboost import Pool, CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8G1QkbbfWsYA"
   },
   "source": [
    "## Definitions for the CatBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5P268w0RWwke"
   },
   "outputs": [],
   "source": [
    "\n",
    "unicode_data_categories = [\n",
    "    \"Cc\",\n",
    "    \"Cf\",\n",
    "    \"Cn\",\n",
    "    \"Co\",\n",
    "    \"Cs\",\n",
    "    \"LC\",\n",
    "    \"Ll\",\n",
    "    \"Lm\",\n",
    "    \"Lo\",\n",
    "    \"Lt\",\n",
    "    \"Lu\",\n",
    "    \"Mc\",\n",
    "    \"Me\",\n",
    "    \"Mn\",\n",
    "    \"Nd\",\n",
    "    \"Nl\",\n",
    "    \"No\",\n",
    "    \"Pc\",\n",
    "    \"Pd\",\n",
    "    \"Pe\",\n",
    "    \"Pf\",\n",
    "    \"Pi\",\n",
    "    \"Po\",\n",
    "    \"Ps\",\n",
    "    \"Sc\",\n",
    "    \"Sk\",\n",
    "    \"Sm\",\n",
    "    \"So\",\n",
    "    \"Zl\",\n",
    "    \"Zp\",\n",
    "    \"Zs\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "column_names = [\n",
    "    \"handle\",\n",
    "    \"timestamp\",\n",
    "    \"text\"\n",
    "]\n",
    "\n",
    "column_names.extend(unicode_data_categories)\n",
    "\n",
    "#List of handles (labels)\n",
    "#Fill with the handles you want to consider in your dataset\n",
    "handles = [\n",
    "\n",
    "]\n",
    "\n",
    "train_file = os.path.realpath(\"./data/train.csv\")\n",
    "val_file = os.path.realpath(\"./data/val.csv\")\n",
    "test_file = os.path.realpath(\"./data/test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "baGty7pSXLCv"
   },
   "source": [
    "## Preprocessing and computing dataset features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wC-YYqQmXMYP"
   },
   "outputs": [],
   "source": [
    "def get_pandas_dataset(input_file, timestamp_mean=None, timestamp_std=None):\n",
    "\n",
    "    pd_dat = pd.read_csv(input_file, names=column_names)\n",
    "\n",
    "    pd_dat = pd_dat[pd_dat.handle.isin(handles)]\n",
    "\n",
    "    if(timestamp_mean is None):\n",
    "         timestamp_mean = pd_dat.timestamp.mean()\n",
    "    \n",
    "    if(timestamp_std is None):\n",
    "        timestamp_std = pd_dat.timestamp.std()\n",
    "\n",
    "    pd_dat.timestamp = (pd_dat.timestamp - timestamp_mean) / timestamp_std\n",
    "\n",
    "    pd_dat[\"handle_index\"] = pd_dat['handle'].map(lambda x: handles.index(x))\n",
    "\n",
    "    pd_dat = pd_dat.reset_index(drop=True)\n",
    "\n",
    "    return pd_dat, timestamp_mean, timestamp_std\n",
    "\n",
    "train_dataset, timestamp_mean, timestamp_std = get_pandas_dataset(train_file)\n",
    "\n",
    "test_dataset, _, _ = get_pandas_dataset(test_file, timestamp_mean=timestamp_mean, timestamp_std=timestamp_std)\n",
    "\n",
    "val_dataset, _, _ = get_pandas_dataset(val_file, timestamp_mean=timestamp_mean, timestamp_std=timestamp_std)\n",
    "\n",
    "def split_inputs_and_outputs(pd_dat):\n",
    "\n",
    "    labels = pd_dat['handle_index'].values\n",
    "\n",
    "    del(pd_dat['handle'])\n",
    "    del(pd_dat['handle_index'])\n",
    "\n",
    "    return pd_dat, labels\n",
    "\n",
    "X_train, labels_train = split_inputs_and_outputs(train_dataset)\n",
    "X_val, labels_val = split_inputs_and_outputs(val_dataset)\n",
    "X_test, labels_test = split_inputs_and_outputs(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otA2nEKNX_N8"
   },
   "source": [
    "## CatBoost model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iy9URje7YAwg"
   },
   "outputs": [],
   "source": [
    "def get_model(catboost_params={}):\n",
    "    cat_features = []\n",
    "    text_features = ['text']\n",
    "\n",
    "    catboost_default_params = {\n",
    "        'iterations': 1000,\n",
    "        'learning_rate': 0.03,\n",
    "        'eval_metric': 'Accuracy',\n",
    "        'task_type': 'GPU',\n",
    "        'early_stopping_rounds': 20\n",
    "    }\n",
    "    \n",
    "    catboost_default_params.update(catboost_params)\n",
    "    \n",
    "    model = CatBoostClassifier(**catboost_default_params)\n",
    "\n",
    "    return model, cat_features, text_features\n",
    "\n",
    "\n",
    "model, cat_features, text_features = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o7pM4RbMYQlA"
   },
   "source": [
    "## CatBoost model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_OAsp6nYXGz"
   },
   "outputs": [],
   "source": [
    "def fit_model(X_train, X_val, y_train, y_val, model, cat_features, text_features, verbose=100):\n",
    "\n",
    "    learn_pool = Pool(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        cat_features=cat_features,\n",
    "        text_features=text_features,\n",
    "        feature_names=list(X_train)\n",
    "    )\n",
    "\n",
    "    val_pool = Pool(\n",
    "        X_val, \n",
    "        y_val, \n",
    "        cat_features=cat_features,\n",
    "        text_features=text_features,\n",
    "        feature_names=list(X_val)\n",
    "    )\n",
    "\n",
    "    model.fit(learn_pool, eval_set=val_pool, verbose=verbose)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = fit_model(X_train, X_val, labels_train, labels_val, model, cat_features, text_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dOksfT2BY5em"
   },
   "source": [
    "## CatBoost model evaluation\n",
    "\n",
    "Also for the CatBoost model, predictions on the test set, a confusion matrix is produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N1VRhDP6YiOn"
   },
   "outputs": [],
   "source": [
    "def predict(X, model, cat_features, text_features):\n",
    "    pool = Pool(\n",
    "        data=X,\n",
    "        cat_features=cat_features,\n",
    "        text_features=text_features,\n",
    "        feature_names=list(X)\n",
    "    )\n",
    "\n",
    "    probs = model.predict_proba(pool)\n",
    "\n",
    "    return probs\n",
    "\n",
    "def check_predictions_on(inputs, outputs, model, cat_features, text_features, handles):\n",
    "    predictions = predict(inputs, model, cat_features, text_features)\n",
    "    labelled_counter = np.zeros((len(handles), len(handles)))\n",
    "\n",
    "    labelled_sequences = np.empty((len(handles), len(handles)), np.object)\n",
    "\n",
    "    for i in range(len(handles)):\n",
    "        for j in range(len(handles)):\n",
    "            labelled_sequences[i][j] = []\n",
    "\n",
    "    tot_wrong = 0\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        predicted = int(predictions[i].argmax())\n",
    "        \n",
    "        true_value = int(outputs[i])\n",
    "\n",
    "        labelled_counter[true_value][predicted] += 1\n",
    "        labelled_sequences[true_value][predicted].append(inputs.get('text').values[i])\n",
    "        \n",
    "        ok = (int(true_value) == int(predicted))\n",
    "        if(not ok):\n",
    "            tot_wrong += 1\n",
    "\n",
    "    return labelled_counter, labelled_sequences, predictions\n",
    "\n",
    "def confusion_matrix(labelled_counter, handles):\n",
    "    the_str = \"\\t\"\n",
    "    for handle in handles:\n",
    "        the_str += handle + \"\\t\"\n",
    "\n",
    "    the_str += \"\\n\"\n",
    "\n",
    "    ctr = 0\n",
    "    for row in labelled_counter:\n",
    "        the_str += handles[ctr] + '\\t'\n",
    "        ctr+=1\n",
    "        for i in range(len(row)):\n",
    "            the_str += str(int(row[i]))\n",
    "            if(i != len(row) -1):\n",
    "                the_str += \"\\t\"\n",
    "        the_str += \"\\n\"\n",
    "\n",
    "    return the_str\n",
    "\n",
    "labelled_counter, labelled_sequences, predictions = check_predictions_on(\n",
    "    X_test,\n",
    "    labels_test,\n",
    "    model,\n",
    "    cat_features,\n",
    "    text_features,\n",
    "    handles\n",
    ")\n",
    "\n",
    "confusion_matrix_string = confusion_matrix(labelled_counter, handles)\n",
    "\n",
    "print(confusion_matrix_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eACMglJZ6Ai"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "To perform some experiments and evaluate the two models, 18 Twitter users were selected and, for each user, a number of tweets and responses to other users' tweets were collected. In total 39786 tweets were collected. The difference in class representation could be eliminated, for example limiting the number of tweets for each label to the number of tweets in the less represented class. This difference, however, was not eliminated, in order to test if it represents an issue for the accuracy of the two trained models.\n",
    "\n",
    "The division of the tweets corresponding to each twitter handle for each file (train, test, validation) is reported in the following table. To avoid policy issues (better safe than sorry), the actual user handle is masked using C_x placeholders and a brief description of the twitter user is presented instead.\n",
    "\n",
    "|Description|Handle|Train|Test|Validation|Sum|\n",
    "|-------|-------|-------|-------|-------|-------|\n",
    "|UK-based labour politician|C_1|1604|492|229|2325|\n",
    "|US-based democratic politician|C_2|1414|432|195|2041|\n",
    "|US-based democratic politician|C_3|1672|498|273|2443|\n",
    "|US-based actor|C_4|1798|501|247|2546|\n",
    "|UK-based actress|C_5|847|243|110|1200|\n",
    "|US-based democratic politician|C_6|2152|605|304|3061|\n",
    "|US-based singer|C_7|2101|622|302|3025|\n",
    "|US-based singer|C_8|1742|498|240|2480|\n",
    "|Civil rights activist|C_9|314|76|58|448|\n",
    "|US-based republican politician|C_10|620|159|78|857|\n",
    "|US-based TV host|C_11|2022|550|259|2831|\n",
    "|Parody account of C_15 |C_12|2081|624|320|3025|\n",
    "|US-based democratic politician|C_13|1985|557|303|2845|\n",
    "|US-based actor/director|C_14|1272|357|183|1812|\n",
    "|US-based republican politician|C_15|1121|298|134|1553|\n",
    "|US-based writer|C_16|1966|502|302|2770|\n",
    "|US-based writer|C_17|1095|305|153|1553|\n",
    "|US-based entrepreneur|C_18|2084|581|306|2971|\n",
    "|Sum||27890|7900|3996|39786|\n",
    "\n",
    "\n",
    "\n",
    "## TensorFlow 2 model\n",
    "\n",
    "The following charts show loss and accuracy vs epochs for train and validation for a typical run of the TF2 model:\n",
    "\n",
    "![loss](img/tf2_train_val_loss.png)\n",
    "![accuracy](img/tf2_train_val_accuracy.png)\n",
    "\n",
    "If the images do not show correctly, they can be found at these links: [loss](https://github.com/icappello/ml-predict-text-author/blob/master/img/tf2_train_val_loss.png) [accuracy](https://github.com/icappello/ml-predict-text-author/blob/master/img/tf2_train_val_accuracy.png)\n",
    "\n",
    "After a few epochs, the model starts overfitting on the train data, and the accuracy for the validation set quickly reaches a plateau.\n",
    "\n",
    "The obtained accuracy on the test set is 0.672\n",
    "\n",
    "## CatBoost model\n",
    "\n",
    "The fit procedure stopped after 303 iterations. The obtained accuracy on the test set is 0.808\n",
    "\n",
    "## Confusion matrices\n",
    "\n",
    "The confusion matrices for the two models are reported [here](https://docs.google.com/spreadsheets/d/17JGDXYRajnC4THrBnZrbcqQbgzgjo0Jb7KAvPYenr-w/edit?usp=sharing), since large tables are not displayed correctly in the embedded github viewer for jupyter notebooks. Rows represent the actual classes, while columns represent the predicted ones.\n",
    "\n",
    "## Summary\n",
    "\n",
    "The CatBoost model obtained a better accuracy overall, as well as a better accuracy on all but one label. No particular optimization was done on the definition of the CatBoost model. The TF2 model could need more data, as well as some changes to its definition, to perform better (comments and pointers on this are welcome). Some variants of the TF2 model were tried: a deeper model with more dense layers, higher dropout rate, more/less units in layers, using only a subset of features, regularization methods (L1, L2, batch regularization), different activation functions (sigmoid, tanh) but none performed significantly better than the one presented.\n",
    "\n",
    "Looking at the results summarized in the confusion matrices, tweets from C_9 clearly represented a problem, either for the under-representation relative to the other classes or for the actual content of the tweets (some were not written in english). Also, tweets from handles C_5 and C_14 were hard to correctly classify for both models, even if they were not under-represented w.r.t other labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "AJdXtG7eQ5tK",
    "bzzxApOjQ_bJ",
    "Dh1eTHn-TY9b",
    "Cc7CEeVgTsWB",
    "8Mo17vnQUBWK",
    "1hygiOHkUQBZ",
    "oEKBxOJ_Wdgy",
    "8G1QkbbfWsYA"
   ],
   "name": "creators_summary.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}